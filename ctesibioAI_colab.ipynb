{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CtesibioAI v0.0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CtesibioAI comes with this file to be used in Colab, so you can train your model using the computational power of Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### follow the steps to do your training\n",
    "\n",
    "* Install the requirements\n",
    "* First you need to pass the data with questions and answers\n",
    "* Train the model\n",
    "* Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You need to pass the data with questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"question\": \"What is a large language model (LLM)?\", \"answer\": \"An LLM is an artificial intelligence model trained on large volumes of text to understand and generate natural language.\"},\n",
    "    {\"question\": \"What are some examples of famous LLMs?\", \"answer\": \"Examples of famous LLMs include GPT, BERT, T5, and ChatGPT.\"},\n",
    "    {\"question\": \"What are LLMs used for?\", \"answer\": \"LLMs are used in tasks such as text generation, machine translation, text summarization, question answering, and more.\"},\n",
    "    {\"question\": \"What does the term 'fine-tuning' mean?\", \"answer\": \"Fine-tuning is the process of adjusting a pre-trained LLM on a specific dataset for a particular application or domain.\"},\n",
    "    {\"question\": \"What is 'tokenization'?\", \"answer\": \"Tokenization is the process of breaking text into smaller units, called tokens, which can be words, subwords, or characters.\"},\n",
    "    {\"question\": \"How do LLMs learn to generate text?\", \"answer\": \"LLMs learn to generate text by predicting the next word or token in a sequence based on patterns in the training text.\"},\n",
    "    {\"question\": \"What is the main challenge in training?\", \"answer\": \"The main challenges include the high computational cost, the need for large datasets, and the difficulty of avoiding biases in the models.\"},\n",
    "    {\"question\": \"What are parameters?\", \"answer\": \"Parameters are adjustable values in the model that determine how it processes and generates text based on input data.\"},\n",
    "    {\"question\": \"What is a transformer?\", \"answer\": \"Transformers are a neural network architecture that uses attention mechanisms to process data sequences, such as text.\"},\n",
    "    {\"question\": \"What is the role of 'pre-training'?\", \"answer\": \"'Pre-training' involves training the model on a large corpus of text to learn general language patterns before fine-tuning it for specific tasks.\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "special_tokens = {\"pad_token\": \"<PAD>\", \"bos_token\": \"<BOS>\", \"eos_token\": \"<EOS>\"}\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "formatted_data = [{\"text\": f\"<BOS>{d['pergunta']} {d['resposta']}<EOS>\"} for d in datas]\n",
    "\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    encoding = tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    encoding[\"labels\"] = encoding[\"input_ids\"].copy()\n",
    "    return encoding\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ctesibioAI-model\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=[],  # Evita integração com wandb ou outros sistemas\n",
    "    evaluation_strategy=\"no\",  # Desabilita avaliação durante o treinamento\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# save model\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./ctesibioAI-model\")\n",
    "tokenizer.save_pretrained(\"./ctesibioAI-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./ctesibioAI-model\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./ctesibioAI-model\")\n",
    "\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.bos_token = \"<BOS>\"\n",
    "tokenizer.eos_token = \"<EOS>\"\n",
    "\n",
    "input_text = \"<BOS>capital do brasil?\" \n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(\n",
    "    inputs,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.pad_token_id,  # Garantir consistência com o treinamento\n",
    "    temperature=0.7,  # Controle de aleatoriedade\n",
    "    top_k=50,         # Considerar apenas os 50 tokens mais prováveis\n",
    "    repetition_penalty=2.0,  # Penalizar repetições\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Ctesibio Response:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now if you want you can download the model to use wherever you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"This piece of code is only used to download the model to your machine through Colab\"\"\"\n",
    "\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "pasta_para_zipar = '.ctesibioAI-model'\n",
    "file_zip = 'ctesibioAI-model.zip' \n",
    "\n",
    "\n",
    "shutil.make_archive(base_name=file_zip.replace('.zip', ''), format='zip', root_dir=pasta_para_zipar)\n",
    "\n",
    "files.download(file_zip)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
